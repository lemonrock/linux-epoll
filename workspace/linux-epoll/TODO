

	Multiple file descriptors problems
		- We own all the file descriptors, eg a pair to be used for a tcp proxy, or a socket, pipe and file fd (for efficiency of file reads)
			- We close one file descriptor, we need to close all file descriptors

		- We own one file descriptor, the other is shared, eg a socket and a shared pipe FD (as a file cache) or Posix MQ FD, etc.
			- We close ours; we may need to close the other


		- One is epoll, one is file
			- file will never block, but could be rather slow for a large read of 100s Mb
				- cache content in several pipe FDs and use tee()

		Problem: close and pending epoll events
			- we close ours in response to an epoll event to us - no problem
			- we close the other file descriptor - there may be pending events for it in the same epoll results loop

		Problem: TLS TCP proxy - two sockets linked back-to-back, both work via TLS (eg a QUIC -> TLS or TLS -> QUIC proxy)
			- if we have the same closure registered for 2 epoll events, when we yield from the closure, we could enter for IO being available on either; we may be waiting on IO for socket 1, but enter with IO for socket 2
				- current design does not tell us the file descriptor
			- we need to register the same closure for 2 sockets, or 2 different closures, in which case, we'll need a buffer shared by both (referenced via Rc<>).
				- if 2 different closures, we need to know that we failed to create the second closure because the 2nd connection 'completed', otherwise we're very stuck.
			- a shared buffer design is not efficient for a classic SOCKS proxy, but, since the initial connection isn't encrypted, a classic SOCKS proxy is probably a bad idea.

		Problem: We need to get an answer from a service - eg DNS, LDAP or a database - before carrying on
			- we yield, but there is already IO available so epoll will never cause us to re-enter
			- this is very much like a thread doing sync / wait



TODO
	- Instantiate each logical-core with a thread.
	- Arenas for coroutine stacks
	- FileDescriptorCOnsumer
        - Fix the problem with additional_data - where does it get passed?
        - We don't want lots and lots of consumer instances; iteration will be crippling.
        - We need to be able to pass other stream-user relevant data, potentially
            - Many possible permuations of StreamingSocketReactor are possible, and they all need to got in an arena and be addressed.
                - Arena of arenas is needed for ListeningSockets and StreamSockets
                - Need to take some sort of key that can be encoded in the arena index
                    - We could pass an event-poll-token-like u64 instead of a RawFd

	// TODO: some sort of error log... capture dropped file descriptors and other nasty errors.
	    - in distribute();
	    - in accept();
	    - in do_initial_input_and_output_and_register_with_epoll_if_necesssary();

    // TODO: copy code from libnuma (numactl) to parse /proc/self/? and extract logical cores bound to the process.
        // Abstract current cpu logical, too.

    // TODO: Write a SOCKS 4a / 5 client Stream impl
    // TODO: Write a TLSSTART Stream impl for LDAP / SMTP

	- how to client and server streaming sockets (perhaps 2 kinds of reactors therefore more arenas)

	- how to manage multiple file descriptors where all are epoll'd to achive a task
		- eg http server that uses a pipe fd (eg as a cache for tee'd data as a zero copy buffer)

	- how to get a result back by spawning, say, an outbound DNS connection
		- we could yield, but how do we get control back on to the coroutine, as no EdgeTriggered Input or Output will be raised?

	- how do use long-lived, multiple request connections such as a database connection or a LDAP connection? And share them amongst many internal coroutines? (with or without a connection pool)

	- new Linux zero copy features

	- new Linux TCP_ULP with sendfile and ?splice - only supported by OpenSSL.

- Outbound things we care about right now:-
	- LDAP
	- DNS over TLS over TCP/QUIC
	- HTTPS REST APIs
	- receiving local UDP syslog
	- sending encrypted syslog over TLS over TCP/QUIC
		- syslog unstructured or structured protocols
		- long-lived HTTP over TLS connection
	- TCP proxies using TLS, eg to support conversion to / from QUIC (require 2 FDs and a magic ring buffer that is thread safe but just producer / consumer)

- In essence, we need the ability to 'wake up' anything currently being handled by epoll.



- We can have far too many producers if we do one per type; it's unsuitably large and the walking would be very painful.

- We need a small integer key to match on in the consumer - and potentially in a subdivided arena
    - if we do the latter we can probably implement most of the socket data 'noise' from FileDescriptor.

- The small integer key is effectively like a TypeId or similar; it can be converted back into a real enum like:-

This is going to require some serious magic. Enum and Union are KNOWN layouts.
Rust's way is to use (Boxed) trait objects (dyn SomeTraitX)


pub trait EnumeratedDatum: Sized
{
    // Hail-Mary hope this lines up...
    const Index: u8;
}

pub type ArenasEnum<Type0, Type2, ...> = EnumeratedData<SimpleArena<Type0>, SimpleArena<Type1>, ...>

pub enum EnumeratedData
<
    Type0: EnumeratedDatum,
    Type1: EnumeratedDatum,
>
{
    Type0(Type0),

    Type1(Type1),

    ...
}

impl EnumeratedData
<
    Type0: EnumeratedDatum,
    Type1: EnumeratedDatum,
>
{
    #[inline(always)]
    pub fn dispatch_move<R>(self, dispatch0: impl FnOnce(Type0) -> R, dispatch1: impl FnOnce(Type1), ...) -> R
    {
        use self::EnumeratedData::*;

        match self
        {
            Type0(enumerated_datum) => dispatch0(enumerated_datum, arguments),

            Type1(enumerated_datum) => dispatch1(enumerated_datum, arguments),

            ...
        }
    }

    #[inline(always)]
    pub fn dispatch_reference<R>(&self, dispatch0: impl Fn(&Type0) -> R, dispatch1: impl Fn(&Type1), ...) -> R
    {
        use self::EnumeratedData::*;

        match *self
        {
            Type0(ref enumerated_datum) => dispatch0(enumerated_datum, arguments),

            Type1(ref enumerated_datum) => dispatch1(enumerated_datum, arguments),

            ...
        }
    }

    #[inline(always)]
    pub fn dispatch_mutable<R>(&mut self, dispatch0: impl Fn(&mut Type0) -> R, dispatch1: impl Fn(&mut Type1), ...) -> R
    {
        use self::EnumeratedData::*;

        match *self
        {
            Type0(ref mut enumerated_datum) => dispatch0(enumerated_datum, arguments),

            Type1(ref mut enumerated_datum) => dispatch1(enumerated_datum, arguments),

            ...
        }
    }
}
